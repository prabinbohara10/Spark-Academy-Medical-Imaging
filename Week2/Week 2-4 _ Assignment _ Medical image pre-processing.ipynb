{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1DKYOvO1nefUFYHjzWAD6oQqY5Jxmf_nd","timestamp":1653398517206},{"file_id":"1y480Sqwvkb0NnwvrxUndBaN-GCSou3BW","timestamp":1653353790227}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment Week 1-4:Image (Pre-)Processing, Quality Assurance prior to DL training.ipynb\n","\n","Created by: **Mikael Del Castillo**, **Malinda Huang**"],"metadata":{"id":"ZC_AykflkH30"}},{"cell_type":"markdown","source":["**Overview**:\n","\n","This assigment will go over:\n","* Basic data exploration\n","* Preprocessing MRI with TorchIO\n","  - Spatial transformations\n","  - Data augmentation\n","  - Intensity transformations\n","\n","\n"],"metadata":{"id":"H5zl4AeDk_DC"}},{"cell_type":"markdown","source":["# Import libraries, packages, and dataset\n","First, lets import all the libraries and dataset needed for this assigment.\n","\n","We will visualize the MRI images of another patient from the same Kaggle dataset that we used in the workshop (https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation)."],"metadata":{"id":"ZtqArpnfkOaM"}},{"cell_type":"code","source":["!pip install torchio==0.18.70 --quiet\n","!curl -s -o colormap.txt https://raw.githubusercontent.com/thenineteen/Semiology-Visualisation-Tool/master/slicer/Resources/Color/BrainAnatomyLabelsV3_0.txt\n","\n","# Loading and processing\n","import os\n","import numpy as np\n","import pandas as pd\n","import nibabel as nib\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","%matplotlib inline \n","\n","# Torch IO\n","import copy\n","import time\n","import pprint\n","\n","import torch\n","import torchio as tio\n","import seaborn as sns; sns.set() # statistical data visualization\n","\n","sns.set_style(\"whitegrid\", {'axes.grid' : False})\n","%config InlineBackend.figure_format = 'retina'\n","torch.manual_seed(14041931)\n","mni = tio.datasets.Colin27()\n","\n","print('TorchIO version:', tio.__version__)\n","print('Last run on', time.ctime())\n"],"metadata":{"id":"knPXzCYKlm68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","project_dir = 'ImagePreprocessing'\n","%cd /content\n","if os.path.exists(project_dir):\n","  shutil.rmtree(project_dir)\n","else: \n","  os.mkdir(project_dir)\n","!git clone https://github.com/MalindaH/ImagePreprocessing.git\n","%cd ImagePreprocessing\n","!unzip /content/ImagePreprocessing/BraTS20_Training_002_t2.nii.zip -d /content/BraTS20_Training_002_t2.nii\n","!rm -rf *.zip"],"metadata":{"id":"rg4TBQfvrFfZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excercise 1: Data Exploration\n","\n","Let's explore our dataset by loading one sample MRI scan!\n","Use the path defined bellow. "],"metadata":{"id":"VDGX04StkTVh"}},{"cell_type":"code","source":["# Use this path when loading your image, make sure you run this section\n","file_path = \"/content/BraTS20_Training_002_t2.nii/BraTS20_Training_002_t2.nii\""],"metadata":{"id":"5QqlYyewosjk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title ## Run me! (Visualization functions from the workshop) \n","\n","#Function to display row of image slices\n","def show_slices(slices):\n","    fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n","    for idx, slice in enumerate(slices):\n","        axes[idx].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n","\n","    axes[0].set_xlabel('Second dim voxel coords.', fontsize=12)\n","    axes[0].set_ylabel('Third dim voxel coords', fontsize=12)\n","    axes[0].set_title('First dimension (i), slice {}'.format(i), fontsize=15)\n","\n","    axes[1].set_xlabel('First dim voxel coords.', fontsize=12)\n","    axes[1].set_ylabel('Third dim voxel coords', fontsize=12)\n","    axes[1].set_title('Second dimension (j), slice {}'.format(j), fontsize=15)\n","  \n","    axes[2].set_xlabel('First dim voxel coords.', fontsize=12)\n","    axes[2].set_ylabel('Second dim voxel coords', fontsize=12)\n","    axes[2].set_title('Third dimension (k), slice {}'.format(k), fontsize=15)\n","\n","# modify above show_slices() fnc to include visualization of coordinate location\n","def add_coords(slices):\n","  fig, axes = plt.subplots(1, len(slices), figsize=(15,15))\n","  for idx, slice in enumerate(slices):\n","    axes[idx].imshow(slice.T, cmap=\"gray\", origin=\"lower\")\n","\n","    axes[0].set_xlabel('Second dim voxel coords.', fontsize=12)\n","    axes[0].set_ylabel('Third dim voxel coords', fontsize=12)\n","    axes[0].set_title('First dimension (i), slice {}'.format(i), fontsize=15)\n","\n","    axes[1].set_xlabel('First dim voxel coords.', fontsize=12)\n","    axes[1].set_ylabel('Third dim voxel coords', fontsize=12)\n","    axes[1].set_title('Second dimension (j), slice {}'.format(j), fontsize=15)\n","  \n","    axes[2].set_xlabel('First dim voxel coords.', fontsize=12)\n","    axes[2].set_ylabel('Second dim voxel coords', fontsize=12)\n","    axes[2].set_title('Third dimension (k), slice {}'.format(k), fontsize=15)  \n","    \n","    # plot the 3D coordinate in red\n","    axes[0].add_patch((patches.Rectangle((j, k), 3, 3, linewidth=2, edgecolor='r', facecolor='none')))\n","    axes[1].add_patch((patches.Rectangle((i, k), 3, 3, linewidth=2, edgecolor='r', facecolor='none')))\n","    axes[2].add_patch((patches.Rectangle((i, j), 3, 3, linewidth=2, edgecolor='r', facecolor='none')))\n","\n","# Functions required for TorchIO \n","def get_bounds(self):\n","    \"\"\"Get image bounds in mm.\n","\n","    Returns:\n","        np.ndarray: [description]\n","    \"\"\"\n","    first_index = 3 * (-0.5,)\n","    last_index = np.array(self.spatial_shape) - 0.5\n","    first_point = nib.affines.apply_affine(self.affine, first_index)\n","    last_point = nib.affines.apply_affine(self.affine, last_index)\n","    array = np.array((first_point, last_point))\n","    bounds_x, bounds_y, bounds_z = array.T.tolist()\n","    return bounds_x, bounds_y, bounds_z\n","\n","def to_pil(image):\n","    from PIL import Image\n","    from IPython.display import display\n","    data = image.numpy().squeeze().T\n","    data = data.astype(np.uint8)\n","    image = Image.fromarray(data)\n","    w, h = image.size\n","    display(image)\n","    print()  # in case multiple images are being displayed\n","\n","def stretch(img):\n","    p1, p99 = np.percentile(img, (1, 99))\n","    from skimage import exposure\n","    img_rescale = exposure.rescale_intensity(img, in_range=(p1, p99))\n","    return img_rescale\n","\n","def show_fpg(\n","        subject,\n","        to_ras=False,\n","        stretch_slices=True,\n","        indices=None,\n","        parcellation=False,\n","        ):\n","    subject = tio.ToCanonical()(subject) if to_ras else subject\n","    def flip(x):\n","        return np.rot90(x) # flip 90 degrees\n","    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n","    if indices is None:\n","        t1_half_shape = torch.Tensor(subject.t1.spatial_shape) // 2\n","        t1_i, t1_j, t1_k = t1_half_shape.long()\n","        t1_i -= 5  # use a better slice\n","\n","        t2_half_shape = torch.Tensor(subject.t2.spatial_shape) // 2\n","        t2_i, t2_j, t2_k = t2_half_shape.long()\n","        t2_i -= 5  # use a better slice\n","    else:\n","        t1_i, t1_j, t1_k = indices\n","    t1bounds_x, t1bounds_y, t1bounds_z = get_bounds(subject.t1)  ###\n","    t2bounds_x, t2bounds_y, t2bounds_z = get_bounds(subject.t2)\n","\n","    orientation = ''.join(subject.t1.orientation)\n","    if orientation != 'RAS':\n","        import warnings\n","        warnings.warn(f'Image orientation should be RAS+, not {orientation}+')\n","    \n","    kwargs = dict(cmap='gray', interpolation='none')\n","    data = subject['t1'].data\n","    slices = data[0, t1_i], data[0, :, t1_j], data[0, ..., t1_k]\n","    if stretch_slices:\n","        slices = [stretch(s.numpy()) for s in slices]\n","    sag, cor, axi = slices\n","    \n","    axes[0, 0].imshow(flip(sag), extent=t1bounds_y + t1bounds_z, **kwargs)\n","    axes[0, 1].imshow(flip(cor), extent=t1bounds_x + t1bounds_z, **kwargs)\n","    axes[0, 2].imshow(flip(axi), extent=t1bounds_x + t1bounds_y, **kwargs)\n","    axes[0, 0].set_title('T1', fontsize=15)\n","    axes[0, 1].set_title('T1', fontsize=15)\n","    axes[0, 2].set_title('T1', fontsize=15)\n","    \n","\n","    kwargs = dict(cmap='gray', interpolation='none')\n","    data2 = subject['t2'].data\n","    slicest2 = data2[0, t2_i], data2[0, :, t2_j], data2[0, ..., t2_k]\n","    if stretch_slices:\n","        slicest2 = [stretch(s.numpy()) for s in slicest2]\n","    sagt2, cort2, axit2 = slicest2\n","    \n","    axes[1, 0].imshow(flip(sagt2), extent=t2bounds_y + t2bounds_z, **kwargs)\n","    axes[1, 1].imshow(flip(cort2), extent=t2bounds_x + t2bounds_z, **kwargs)\n","    axes[1, 2].imshow(flip(axit2), extent=t2bounds_x + t2bounds_y, **kwargs)\n","    axes[1, 0].set_title('T2', fontsize=15)\n","    axes[1, 1].set_title('T2', fontsize=15)\n","    axes[1, 2].set_title('T2', fontsize=15)\n","\n","def show_rbf(\n","        subject,\n","        to_ras=False,\n","        stretch_slices=True,\n","        indices=None,\n","        intensity_name='t1',\n","        parcellation=True,\n","        ):\n","    subject = tio.ToCanonical()(subject) if to_ras else subject\n","    def flip(x):\n","        return np.rot90(x)\n","    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n","    if indices is None:\n","        half_shape = torch.Tensor(subject.spatial_shape) // 2\n","        i, j, k = half_shape.long()\n","        i -= 5  # use a better slice\n","    else:\n","        i, j, k = indices\n","    bounds_x, bounds_y, bounds_z = get_bounds(subject.t1)  ###\n","\n","    orientation = ''.join(subject.t1.orientation)\n","    if orientation != 'RAS':\n","        import warnings\n","        warnings.warn(f'Image orientation should be RAS+, not {orientation}+')\n","    \n","    kwargs = dict(cmap='gray', interpolation='none')\n","    data = subject[intensity_name].data\n","    slices = data[0, i], data[0, :, j], data[0, ..., k]\n","    if stretch_slices:\n","        slices = [stretch(s.numpy()) for s in slices]\n","    sag, cor, axi = slices\n","    \n","    axes[0, 0].imshow(flip(sag), extent=bounds_y + bounds_z, **kwargs)\n","    axes[0, 1].imshow(flip(cor), extent=bounds_x + bounds_z, **kwargs)\n","    axes[0, 2].imshow(flip(axi), extent=bounds_x + bounds_y, **kwargs)\n","\n","    kwargs = dict(interpolation='none')\n","    data = subject.seg.data\n","    slices = data[0, i], data[0, :, j], data[0, ..., k]\n","    if parcellation:\n","        sag, cor, axi = [color_table.colorize(s.long()) if s.max() > 1 else s for s in slices]\n","    else:\n","        sag, cor, axi = slices\n","    axes[1, 0].imshow(flip(sag), extent=bounds_y + bounds_z, **kwargs)\n","    axes[1, 1].imshow(flip(cor), extent=bounds_x + bounds_z, **kwargs)\n","    axes[1, 2].imshow(flip(axi), extent=bounds_x + bounds_y, **kwargs)\n","    \n","    plt.tight_layout()\n","\n","\n","class ColorTable:\n","    def __init__(self, colors_path):\n","        self.df = self.read_color_table(colors_path)\n","\n","    @staticmethod\n","    def read_color_table(colors_path):\n","        df = pd.read_csv(\n","            colors_path,\n","            sep=' ',\n","            header=None,\n","            names=['Label', 'Name', 'R', 'G', 'B', 'A'],\n","            index_col='Label',\n","        )\n","        return df\n","\n","    def get_color(self, label: int):\n","        \"\"\"\n","        There must be nicer ways of doing this\n","        \"\"\"\n","        try:\n","            rgb = (\n","                self.df.loc[label].R,\n","                self.df.loc[label].G,\n","                self.df.loc[label].B,\n","            )\n","        except KeyError:\n","            rgb = 0, 0, 0\n","        return rgb\n","\n","    def colorize(self, label_map: np.ndarray) -> np.ndarray:\n","        rgb = np.stack(3 * [label_map], axis=-1)\n","        for label in np.unique(label_map):\n","            mask = label_map == label\n","            color = self.get_color(label)\n","            rgb[mask] = color\n","        return rgb\n","\n","color_table = ColorTable('/content/colormap.txt')"],"metadata":{"id":"re0QX1iDYM1y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Your tasks: \n","1. Load the data file and get the image data. Print the data type and its size.\n","2. Display the values of a 5x5 voxel of the center of the dataframe\n","3. Visualize the slices over the first, second and third dimensions of the image array using the center coordinates\n"],"metadata":{"id":"SZdN1SXJqaaF"}},{"cell_type":"code","source":["# Type your code here \n"],"metadata":{"id":"WSLW-nvwpRKB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excersice 2: Preprocessing MRI with TorchIO: Spatial Transformations\n","\n","[TorchIO](https://torchio.readthedocs.io) is an open-source Python library for efficient loading, preprocessing, augmentation and patch-based sampling of 3D medical images in deep learning, following the design of PyTorch.\n","\n","Load the dataset from TorchIO and display them.\n"],"metadata":{"id":"0JYlCHbH1gv4"}},{"cell_type":"code","source":["fpg = tio.datasets.FPG(load_all = True)\n","print('Sample subject:', fpg)\n","print(fpg.t1)\n","print(fpg.t2)\n","\n","show_fpg(fpg)"],"metadata":{"id":"q0iZ9tb-tgmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Your tasks:\n","1. Fix the orientation of the images. Use the canonical transformation to ensure consistent orientation across our dataset.\n","2. Fix the spacing by putting our dataset in the same standard space (the MNI space)\n","3. Crop or pad the images to the `target_shape = 150, 190, 170`\n","4. If the dataset is too heavy you can always downsample. You can make your code to run faster but the resolution of your images is decreased. Try downsapling by 5 and then by 2. Comment on the difference you see."],"metadata":{"id":"JUYn5Z3yZ5nw"}},{"cell_type":"code","source":["# Type your code here\n"],"metadata":{"id":"lP5r4UrRwCLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excersice 3: Preprocessing MRI with TorchIO: Data Augmentation"],"metadata":{"id":"rXgNO0PZf_EF"}},{"cell_type":"markdown","source":["Data augmentation helps increase the number of data images we can feed the model and helps the model generalize.\n","\n","## Your tasks:\n","1. Add random anisotropy to our dataset and visualize it\n","2. Add random affine to our dataset and visualize it\n","3. Add random flip along the anterior-posterior axis with probability = 0.5 to our dataset and visualize it\n","4. Add random elastic deformation with `max_displacement = 10, 20, 15` to our dataset and visualize it"],"metadata":{"id":"DHawkOoNdFnH"}},{"cell_type":"code","source":["# Type your code here\n"],"metadata":{"id":"A1oF8noWdABS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Excersice 4: Preprocessing MRI with TorchIO: Intensity Transforms"],"metadata":{"id":"jh4INlapf6cL"}},{"cell_type":"markdown","source":["## Your tasks:\n","1. Rescale the intensity of our dataset to the range (0,1) and ignore all pixel values outside the 1% and 99% percentiles. Visualize it\n","2. Add random blur to our dataset and visualize it\n","3. Add random noise to our dataset and visualize it\n","4. Add k-space transformations to our dataset including random spike, random ghosting, and random motion, and visualize them"],"metadata":{"id":"GQ__l7UBgG62"}},{"cell_type":"code","source":["# Type your code here\n"],"metadata":{"id":"Q4GWTZFtc_6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 5 (bonus): Apply a chain of transformations\n","\n","## Your task:\n","Use Compose to chain some of the above transformations of your choice (from exercises 2-4) and apply them on our dataset\n","- Be mindful of RAM limits\n","- Use `OneOf` and the `p` kwarg wisely"],"metadata":{"id":"W-3bnGy8j0nT"}},{"cell_type":"code","source":["# Type your code here\n"],"metadata":{"id":"GL6-K3KJj20K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Congratulations! You know how to load, display and pre-process MRI images"],"metadata":{"id":"SXlcplMIBC3Q"}},{"cell_type":"markdown","source":["#Assignment Solution\n","\n","[solution](https://colab.research.google.com/drive/1NFj_xhQmqEM5aS36Lc3CHv9o1_zBtxOG?usp=sharing)\n","\n","Only look at the solutions when you have attempted the exercises"],"metadata":{"id":"jMCpQ11mjgW8"}},{"cell_type":"markdown","source":["# References:\n","\n","[BraTS2020 Dataset](https://www.kaggle.com/datasets/awsaf49/brats20-dataset-training-validation) from Kaggle.\n","\n","\n"],"metadata":{"id":"DVlwPd0dMsJP"}}]}